{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f6449a",
   "metadata": {},
   "source": [
    "# Quiz: Foundations of Robotics - Managing the world complexity: from linear regression to deep learning.\n",
    "\n",
    "This \"quiz\" is in fact a small python exercice of practical interest.\n",
    "\n",
    "First, you will use a pre-trained `YOLO` CNN to detect things like people and cars in real camera images.\n",
    "\n",
    "Then, you will familiarize with `Gym` and use the `stable baselines` RL framework to train a simple MLP policy.\n",
    "\n",
    "_(NB: to execute a jupyter cell, you can press SHIFT+RETURN)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64e3b0",
   "metadata": {},
   "source": [
    "### Installation of the required python libraries:\n",
    "\n",
    "_(NB: this requires an Internet connection and may take some time, remove the -qqq if you need troubleshooting)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfce46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python -qqq\n",
    "!pip install numpy -qqq\n",
    "!pip install gym -qqq\n",
    "!pip install pyglet -qqq\n",
    "!pip install stable-baselines3 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9daaa",
   "metadata": {},
   "source": [
    "### Libraries used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  # to read CSV files, such as the classes file for YOLO\n",
    "import cv2  # for efficient image manipulation\n",
    "import numpy as np  # for efficient array manipulation\n",
    "import gym  # RL environments\n",
    "from stable_baselines3 import SAC  # deep RL training algorithm\n",
    "\n",
    "# other libraries\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fffb",
   "metadata": {},
   "source": [
    "## 1) You Only Look Once (YOLO)\n",
    "\n",
    "In this first exercice, you will be using a YOLO pre-trained CNN for detecting other agents in camera images. Before we start, check that all the following files are in the same folder as the notebook:\n",
    "\n",
    "- `yolov3.weights`: this file contains pre-trained parameters for a huge YOLO-v3 neural network, trained on a dataset called COCO (Common Objects in COntext).\n",
    "\n",
    "- `yolov3.cfg`: this file describes the architecture of the big YOLO-v3 neural network.\n",
    "\n",
    "- `yolov3-tiny.weights`: contains parameters of a much lighter variant of the YOLO-v3 architecture, also trained on the COCO dataset.\n",
    "\n",
    "- `yolov3-tiny.cfg`: describes the tiny variant.\n",
    "\n",
    "(NB: these two models are meant to process images of size 320 x 320, but this notebook will handle other sizes automatically)\n",
    "\n",
    "- `coco.name`: contains the names of all 80 classes present in the COCO dataset (you can read the file with a text editor like notepad).\n",
    "\n",
    "- `street_image.jpg`: example camera image for testing our YOLO pre-trained models.\n",
    "\n",
    "### Question 1.0\n",
    "**Q:** Is YOLO supervised or unsupervised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3c909",
   "metadata": {},
   "source": [
    "**A:** (Your text here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3628c",
   "metadata": {},
   "source": [
    "### Question 1.1\n",
    "**Q:** Both pre-trained YOLO models defined by the aforementionned files are unable to detect one of the following classes, which one is it, and why?\n",
    "- A car\n",
    "- A person\n",
    "- An elephant\n",
    "- A banana\n",
    "- A pineapple\n",
    "- A pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8f875",
   "metadata": {},
   "source": [
    "**A:** (Your text here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51381a3",
   "metadata": {},
   "source": [
    "### Useful functions:\n",
    "\n",
    "The following functions are provided for your convenience, so you don't need to learn YOLO implementation details. Just read the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69168adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo(configuration, weights, classes):\n",
    "    \"\"\"Extracts a pre-trained YOLO model and a list of class names from files.\n",
    "    \n",
    "    Args:\n",
    "        configuration: name of the configuration file (e.g., 'yolov3.cfg')\n",
    "        weights: name of the parameters file (e.g., 'yolov3.weights')\n",
    "        classes: name of the classes file (e.g., 'coco.names')\n",
    "    \n",
    "    Returns:\n",
    "        yolo: the pre-trained model\n",
    "        yolo_classes: the list of available class names\n",
    "    \"\"\"\n",
    "    with open(classes, 'r') as f:\n",
    "        yolo_classes = [item[0] for item in csv.reader(f)]\n",
    "    yolo = cv2.dnn.readNetFromDarknet(configuration, weights)\n",
    "    yolo.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "    yolo.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "    return yolo, yolo_classes\n",
    "\n",
    "def yolo_inference(yolo_model, yolo_classes, image,\n",
    "                   yolo_width=320, yolo_height=320,\n",
    "                   confidence_threshold=0.5, nms_threshold=0.3):\n",
    "    \"\"\"Performs a forward propagation in a pre-trained YOLO model.\n",
    "    \n",
    "    Args:\n",
    "        yolo_model: pre_trained model (output of get_yolo())\n",
    "        yolo_classes: list of class names (output of get_yolo())\n",
    "        image: input cv2 image\n",
    "        yolo_width: width expected by the model\n",
    "        yolo_height: height expected by the model\n",
    "        confidence_threshold: detections are reported only above this threshold\n",
    "        nms_threshold: NMS threshold for filtering out irrelevant boxes\n",
    "    \n",
    "    Returns:\n",
    "        classes: classes of all detected objects\n",
    "        scores: confidence scores of all detected object\n",
    "        boxes: x, y, width and height of all detected bounding boxes\n",
    "    \"\"\"\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    blob_image = cv2.dnn.blobFromImage(image, 1/255,\n",
    "                                       (yolo_width, yolo_height),\n",
    "                                       [0, 0, 0], 1, crop=False)\n",
    "    yolo_model.setInput(blob_image)\n",
    "    layer_names = yolo_model.getLayerNames()\n",
    "    out_layers = yolo_model.getUnconnectedOutLayers()\n",
    "    out_layer_names = [layer_names[i - 1] for i in out_layers]\n",
    "    outputs = yolo_model.forward(out_layer_names)\n",
    "    res_classes = []\n",
    "    res_scores = []\n",
    "    res_boxes = []\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_index = np.argmax(scores)\n",
    "            class_confidence = float(scores[class_index])\n",
    "            if class_confidence > confidence_threshold:\n",
    "                box_width = int(detection[2] * width)\n",
    "                box_height = int(detection[3] * height)\n",
    "                box_x = int(detection[0] * width - box_width / 2)\n",
    "                box_y = int(detection[1] * height - box_height / 2)\n",
    "                res_classes.append(yolo_classes[class_index])\n",
    "                res_scores.append(class_confidence)\n",
    "                res_boxes.append([box_x, box_y, box_width, box_height])\n",
    "    res_classes = np.array(res_classes)\n",
    "    res_scores = np.array(res_scores)\n",
    "    res_boxes = np.array(res_boxes)\n",
    "    if len(res_classes) != 0:\n",
    "        valid_boxes = cv2.dnn.NMSBoxes(res_boxes, res_scores,\n",
    "                                       confidence_threshold, nms_threshold)\n",
    "        res_classes = res_classes[valid_boxes]\n",
    "        res_scores = res_scores[valid_boxes]\n",
    "        res_boxes = res_boxes[valid_boxes]\n",
    "    return res_classes, res_scores, res_boxes\n",
    "\n",
    "def draw_boxes(image, classes, scores, boxes):\n",
    "    \"\"\"Draws bounding boxes in image from the output of yolo_inference()\n",
    "    \n",
    "    Args:\n",
    "        image: the cv2 image to draw boxes onto\n",
    "        classes: 1st output of yolo_inference()\n",
    "        scores: 2nd output of yolo_inference()\n",
    "        boxes: 3rd output of yolo_inference()\n",
    "    \"\"\"\n",
    "    assert len(classes) == len(scores) == len(boxes), \"Dimensions don't match.\"\n",
    "    for i, c in enumerate(classes):\n",
    "        s = scores[i]\n",
    "        b = boxes[i]\n",
    "        x, y, w, h = b\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(img=image,\n",
    "                    text=f\"{c.upper()}: {int(s*100)}%\",\n",
    "                    org=(x, y-10),\n",
    "                    fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale=1.0,\n",
    "                    color=(255, 0, 0),\n",
    "                    thickness=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5cadc",
   "metadata": {},
   "source": [
    "### \"Big\" YOLO model on a street picture:\n",
    "\n",
    "The `street_image.jpg` picture is a camera image featuring a visually complex street scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e188407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the image:\n",
    "\n",
    "image = cv2.imread('street_image.jpg')\n",
    "\n",
    "print(\"Showing image. Focus the window and press any key to close.\")\n",
    "\n",
    "# Show the scene:\n",
    "\n",
    "cv2.imshow('capture', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa641ed",
   "metadata": {},
   "source": [
    "Let us use YOLO to detect people and cars in this scene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('street_image.jpg')\n",
    "\n",
    "yolo, yolo_classes = get_yolo(configuration='yolov3.cfg',\n",
    "                              weights='yolov3.weights',\n",
    "                              classes='coco.names')\n",
    "classes, scores, boxes = yolo_inference(yolo, yolo_classes, image)\n",
    "draw_boxes(image, classes, scores, boxes)\n",
    "\n",
    "print(\"Showing detected objects.\")\n",
    "\n",
    "cv2.imshow('capture', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6820cb",
   "metadata": {},
   "source": [
    "Pretty neat, right?\n",
    "\n",
    "### Question 1.2\n",
    "**Q:** Now, as an exercice, do the same thing, but using your webcam (or another video stream) to process frames in real time, drawing bounding boxes, and displaying them on top of the stream. You can use the following code base:\n",
    "\n",
    "```python\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Press any key to close the window.\")\n",
    "while True:\n",
    "    success, image = capture.read()\n",
    "    assert success\n",
    "    \n",
    "    # your code here\n",
    "    # (...)\n",
    "    \n",
    "    cv2.imshow('capture', image)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k != -1:\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "del(capture)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: (Your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7de0be",
   "metadata": {},
   "source": [
    "### Question 1.3\n",
    "**Q:** Err, that was quite laggy. And your computer CPU is pretty fast! Imagine the same model running on an embedded system... Come up with a solution to accelerate the framerate of our little application. What is the tradeoff here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fddf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: (Your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c8344",
   "metadata": {},
   "source": [
    "**A:** (Your text here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972961d0",
   "metadata": {},
   "source": [
    "Nevertheless, keep in mind that YOLO is likely not using its capacity optimally (and in general, no deep learning model is). Coming up with better architectures (i.e., better inductive biases) and better training algorithms can greatly improve the performance of similar small models in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c829f98",
   "metadata": {},
   "source": [
    "## 2) Soft Actor-Critic (SAC)\n",
    "\n",
    "In this second exercice, we will familiarize with `Gym` environments, and use the `SAC` deep RL algorithm to solve a simple continuous control environment called `Pendulum-v0`. SAC is readily implemented in the `Stable Baselines 3` RL framework.\n",
    "\n",
    "Gym is a simple interface wrapping RL environments. It is designed so that readily implemented RL algorithms can easily interact with these environments. The main methods in a Gym environment are `reset()`, which sets the environment in an initial state and outputs an initial observation, and `step(action)`, which takes an action as input and outputs a new observation, an instantaneous reward, a done signal and an information dictionary.\n",
    "\n",
    "Gym comes with a number of readily available environments, such as `Pendulum-v0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c58940",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce245e",
   "metadata": {},
   "source": [
    "`Pendulum-v0` is a classic continuous control task in which one tries to balance an inverted pendulum up against gravity.\n",
    "\n",
    "Let us examine the _observation space_ of this environment, i.e., the range of possible observations returned by `reset` and `step`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59276edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6573d",
   "metadata": {},
   "source": [
    "This observation space is a `Box` of 3 float values, comprised between `[-1, -1, -8]` and `[1, 1, 8]`. In other words, observations for this environment are arrays of 3 floats:\n",
    "- an angle cosine (between -1 and 1 rad)\n",
    "- an angle sine (between -1 and 1 rad)\n",
    "- an angular speed (between -8 and 8 rad/s)\n",
    "\n",
    "Now, let us examine the _action space_, i.e., the range of possible actions taken as input by the `step` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568a0dd",
   "metadata": {},
   "source": [
    "This action space is very simple: unidimensional, between -2 and 2. Actions for this environment are arrays of 1 single float:\n",
    "- the applied torque (between -2 and 2)\n",
    "\n",
    "The `render` method enables visualizing the environment. Let us apply actions sampled randomly in the action space of `Pendulum-v0` and see what happens:\n",
    "\n",
    "(_The visualization window may be hidden by the notebook, if so, bring it to the front._ **Note: do not close the window otherwise python will crash, just wait until it is closes itself.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79586128",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()  # sets the envrionment to an initial state\n",
    "for _ in range(200):  # let us perform 200 time steps\n",
    "    act = env.action_space.sample()  # samples a random action\n",
    "    obs, rew, done, info = env.step(act)  # applies the action\n",
    "    env.render()  # renders the environment\n",
    "env.close()  # closes the rendering window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74873ec2",
   "metadata": {},
   "source": [
    "We have applied random torques at each time step and the pendulum has done crazy things in response.\n",
    "\n",
    "### Question 2.1\n",
    "**Q:** Instead of applying random actions, **try** to design your own hard-coded policy, i.e., use `obs` to compute clever actions that balance the pendulum up against gravity. Do not spend too much time on this, as this is just to give you a feeling of how difficult this \"simple\" task is. In fact, no correction will be provided for this question. You may use the following code base:\n",
    "\n",
    "```python\n",
    "def policy(observation):\n",
    "    \n",
    "    cos_theta, sin_theta, theta_dot = observation\n",
    "    torque = 0.001  # use cos_theta, sin_theta and theta_dot to adapt this\n",
    "    # (NB: do not mind the display bug when torque is exactly 0)\n",
    "\n",
    "    action = np.array([torque,])\n",
    "    return action\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(200):\n",
    "    act = policy(obs)\n",
    "    obs, rew, done, info = env.step(act)\n",
    "    env.render()\n",
    "env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872189bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: (Your code here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae61323",
   "metadata": {},
   "source": [
    "Hardcoding your own effetive policy would require a non-negligible amount of engineering: probably you would need to make the pendulum oscilliate until it is close enough to the goal position, where you would need to switch to some PID-like controller for stabilization. Deep RL is an interesting alternative, since it is able to automatically find a working policy by trial-and-error instead.\n",
    "\n",
    "`Stable baselines 3` is a framework that readily implements many state-of-the-art deep RL algorithms. Because our environment is wrapped in the `Gym` interface, we can easily use this framework to train a policy in our environment. For instance, we can directly try the `SAC` algorithm as follows:\n",
    "\n",
    "_(NB: this will take some time until \"total_timesteps\" reaches 10000)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC using an MLP policy in our environment:\n",
    "\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train our MLP policy for 10000 training steps:\n",
    "\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149266c8",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "**Q:** Examine the `ep_rew_mean` metric. What do you think this metric represents? How did it evolve during training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1afda4a",
   "metadata": {},
   "source": [
    "**A:** (Your text here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce709d26",
   "metadata": {},
   "source": [
    "We can now use the `predict` method of our `SAC` object as our trained policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602415d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1aa80",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "**Q:** Finally, test this policy in our environment for 1000 time steps (i.e., 5 episodes).\n",
    "\n",
    "_(NB: when an episode is complete, the `done` signal becomes `True`, and `reset` must be called)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: (Your code here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
